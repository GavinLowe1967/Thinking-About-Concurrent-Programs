A \emph{concurrent datatype} is a datatype, for example, a queue, stack, set
or mapping, that can be safely accessed concurrently by multiple threads.
Operation invocations should appear to take place in a one-at-a-time order,
without interference.

Threads that use the concurrent datatype can act much as they would with a
sequential datatype: the implementer of the threads does not need to think
much about concurrency.  

The implementer of the concurrent datatype \emph{does} have to think about
concurrency, and ensure different operation calls do not interfere with one
another.  But that concurrency is local, often inside a single object: local
reasoning is much easier than global reasoning.  And there's a lot of scope
for re-using concurrent datatypes.

Datatype-based concurrent programming encapsulates \emph{all} the concurrency
within a small number of concurrent datatypes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Example: a concurrent queue}
\label{sec:total-queue}

To illustrate some of the ideas, we will implement a concurrent queue.  First,
we should ask: what interface should a concurrent queue have?  A sequential
queue would typically have an interface like:
%
\begin{mysamepage}
\begin{scala}
trait Queue[T]{
  /** Enqueue x. */
  def enqueue(x: T): Unit

  /** Dequeue a value.  Precondition: the queue is not empty. */
  def dequeue(): T

  /** Is the queue empty? */
  def isEmpty: Boolean
}
\end{scala}
\end{mysamepage}
%
The |dequeue| operation has a non-trivial precondition, so client code that
uses the queue would be expected to check this first, for example, via code
such as:
\begin{scala}
  if(queue.isEmpty){ ... /* Handle the empty queue. */ } 
  else{ val x = queue.dequeue(); ... /* Do something with £x£. */ }
\end{scala}

However, this won't work with multi-threaded code: thread~$t$ might check that
the queue is non-empty; but then other threads might empty the queue before
$t$ attempts the |dequeue|, at which point the precondition is violated.  This
is a time-of-check to time-of-use (TOCTTOU) problem: there is a delay between
the thread checking the relevant precondition and performing the operation that
depends upon it, during  which the precondition becomes false; several
exercises in Chapter~\ref{chap:intro} considered the same issue.

We saw in Chapter~\ref{chap:clientServer} that there are two main ways of
dealing with operations that have a non-trivial precondition.  One way is to
return a special value to indicate that the precondition does not hold.  In
Scala, it is natural to use an |Option| value, with |None| indicating that the
precondition does not hold.  In some circumstances, it might be appropriate
(and more efficient) to use |null| for this, if |null| can never be returned
when the precondition does hold.  An alternative, is for the operation to
throw an exception, and expect the thread to catch it.  Operations that treat
preconditions in this way are called \emph{total}: they can take effect in any
state.

The other way to deal with the case that the precondition does not hold is to
block the thread until the precondition becomes true.  Such operations are
called \emph{partial}.

We will start with a total concurrent queue, with the following interface,
where the |dequeue| operation returns an |Option| type, with |None| used to
indicate am empty queue. 
%
\begin{mysamepage}
\begin{scala}
/** A total queue. */
trait TotalQueue[T]{
  /** Enqueue x. */
  def enqueue(x: T): Unit

  /** Dequeue a value.  Returns £None£ if the queue is empty. */
  def dequeue(): Option[T]

  /** Shut down the queue. */
  def shutdown: Unit
}
\end{scala}
\end{mysamepage}

A thread that performs a |dequeue| should handle both cases, for example, via
code such as:
\begin{scala}
  queue.dequeue() match{
    case Some(x) => ... // Do something with £x£.
    case None => ... // Handle the empty queue.
  }
\end{scala}

Figure~\ref{fig:total-queue-server} gives a straightforward implementation of
a total queue that encapsulates a server.  But we could also implement the
queue using one of the techniques we'll see later in the course.  The server
stores the queue itself (using a |Queue| from the Scala API).  Clients use
channels to cause the server to enqueue and dequeue values.  Note that the
server handles operations in a one-at-a-time way, preventing operations from
interfering with one another.  We also include a |shutdown| operation, to
provide a way to terminate the server thread, and so allow garbage collection.

%%%%%

\begin{figure}
\begin{scala}
class ServerTotalQueue[T] extends TotalQueue[T]{
  // Channels used for enqueueing and dequeueing.
  private val enqueueChan = new SyncChan[T]
  private val dequeueChan = new SyncChan[Option[T]]

  def enqueue(x: T) = enqueueChan!x

  def dequeue(): Option[T] = dequeueChan?()

  private def server = thread("ServerTotalQueue"){
    val queue = new scala.collection.mutable.Queue[T]
    serve(
      enqueueChan =?=> { x => queue.enqueue(x) }
      | dequeueChan =!=> { 
          if(queue.nonEmpty) Some(queue.dequeue()) else None 
        }
    )
  }

  fork(server)

  def shutdown() = { enqueueChan.close; dequeueChan.close }
\end{scala}
\caption{A total queue implemented using a server.}
\label{fig:total-queue-server}
\end{figure}

%%%%%

\begin{instruction}
Study the details of the implementation.
\end{instruction}


