In this chapter we will study a particular style of data parallel programming
where the threads proceed \emph{synchronously}.  The program proceeds in
\emph{rounds}.  In each round, each thread performs some computation.
However, at the end of each round, each thread needs to wait for the other
threads to finish the round before they all proceed to the next round: this
requires a global synchronisation.

%%%%%

Typically, each thread will operate on one section of the data, but  may
need to read data updated by other threads.  The synchronisation can be used
to ensure that one thread obtains the updates to the data made by other
threads on the previous round.

The data can be distributed between threads by two different techniques.
%
\begin{itemize}
\item
By sending messages; this works well when each piece of data has to be
passed to only a few other threads;

\item
By writing to shared variables.
\end{itemize}

%These algorithms are sometimes known as heart-beat algorithms.

%%%%%

Applications include: image processing, where different threads operate on
different parts of the image; solving differential equations, for example in
weather forecasting or fluid dynamics, where different threads operate on
different areas; matrix calculations, where different threads operate on
different parts of the matrix.

%%%%%


The global synchronisation at the end of each round is sometimes known as a
\emph{barrier synchronisation}, since it represents a barrier than no thread
may pass until all have reached that point.

Suppose we have |p| threads with identities $\interval{\sm 0}{\sm p}$.  Then a
suitable barrier synchronisation object may be created in SCL by:
%
\begin{scala}
  val barrier = new Barrier(p)
\end{scala}
%
A thread with identity~|me| performs the barrier synchronisation by executing%
%
\begin{scala}
  barrier.sync(me)
\end{scala}
%
No call to \SCALA{sync} will return until all \SCALA{p} threads have called
it. 

%%%%%

\begin{figure}
\begin{scala}
class ServerBarrier(p: Int){
  private val arrive = new SyncChan[Unit]
  private val leave = new SyncChan[Unit]

  def sync(me: Int) = { arrive!(); leave?() }

  private def server = thread{
    while(true){
      for(i <- 0 until p) arrive?()
      for(i <- 0 until p) leave!()
    }
  }

  fork(server)
}
\end{scala}
\caption{A simple implementation of a barrier synchronisation.}
\label{fig:serverBarrier}
\end{figure}

Figure~\ref{fig:serverBarrier} gives a possible implementation of a barrier
using a server.  The server waits to receive a message on channel |arrive|
from each of the |p|~threads before sending then a message on channel~|leave|,
telling them they can continue.

The above implementation means that each synchronisation takes time $O(\sm p)$,
assuming all the threads call |sync| at the same time.  In fact, the
implementation in SCL is more sophisticated, and allows each synchronisation
to take time $O(\log \sm p)$.  Exercise~\ref{ex:barrierLog} asks you to
implement a barrier with this property.

One important feature of a barrier synchronisation is that it also ensures
cache consistency between different threads.  If one thread writes to a shared
variable before a synchronisation, and another thread reads that variable
after the synchronisation, then the read is guaranteed to see the effects of
the write.  Further, compiler optimisations are not allowed to break this
property.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Example: simulating astronomical bodies}

In this section, we will implement a concurrent simulation of astronomical
bodies, such as stars, planets and moons, moving relative to one another under
gravity.  Each concurrent worker will be responsible for updating the state of
some of the bodies in each step of the simulation.  However, each worker will
need to read the states of all of the bodies.  We will use barrier
synchronisations to coordinate the threads and to avoid races.

We will need to record the position and velocity of each astronomical body.
Each of these is a vector in three-dimensional space.  The support class
|Vector|, in Figure~\ref{fig:vector}, models such vectors.

%%%%%

\begin{figure}
\begin{scala}
/** A vector in three-dimensional space. */
case class Vector(x: Double, y: Double, z: Double){
  /** The length of this vector. */
  def length = Math.sqrt(x*x + y*y + z*z)

  /** This plus £v£. */
  def + (v: Vector) = Vector(x+v.x, y+v.y, z+v.z)

  /** The minus £v£. */
  def - (v: Vector) = Vector(x-v.x, y-v.y, z-v.z)

  /** This times £s£. */
  def * (s: Double) = Vector(x*s, y*s, z*s)
}

object Gravity{
  /** Gravitational constant. */
  val G = 6.6743E-11

  /** The time for one step of the simulation. */
  val Timestep = 100_000
}
\end{scala}
\caption{The class {\scalashape Vector} and the object {\scalashape Gravity}.}
\label{fig:vector}\label{fig:Gravity}
\end{figure}

%%%%%

Given two bodies, of mass $m_1$ and $m_2$ at a distance~$d$ from each other,
each exerts a force on the other of magnitude $G \times m_1 \times m_2 / d^2$,
where $G \approx 6.6743 \times 10^{-11}$ is the gravitational constant.  These
are attractive forces: the force on each body is directly towards the other.
The force on each body gives an acceperation equal to the force divided by the
body's mass.

The object |Gravity| in Figure~\ref{fig:Gravity} defines the
gravitational constant~|G|, and also the time (in seconds) represented by each
step of the simulation. 

%%%%%

\begin{figure}
\begin{scala}
/** A single astronomical body, with mass £mass£, initial position £position0£,
  * and initial velocity £velocity0£. */
class AstronomicalBody(val mass: Double, position0: Vector, velocity0: Vector){
  /** The body's current position. */
  private var position = position0

  /** The body's current velocity. */
  private var velocity = velocity0

  /** Update the velocity of this based on the gravitational attraction from
    * £other£. */
  def updateVel(other: AstronomicalBody) = {
    val towards = other.position-position // Vector to £other£.
    val d = towards.length // Distance to £other£.
    val force = Gravity.G*mass*other.mass/(d*d) // Force on this.
    val dv = towards*(force/mass/d*Gravity.Timestep) // Change in velocity.
    velocity += dv
  }

  /** Move this for one timestep.  Return the new position. */
  def move() = { position += velocity*Gravity.Timestep; position }
}
\end{scala}
\caption{The class {\scalashape
    AstronomicalBody}.} 
\label{fig:AstronomicalBody}
\end{figure}

%%%%%

Each object of the class |AstronomicalBody| represents an astronomical body.
The object records the body's mass, position and velocity.  The operation
|updateVel(other)| updates the velocity of the body based on gravitational
attraction from |other|, for one step of the simulation.  The operation |move|
updates the position of the body based on its current velocity, for one step
of the simulation, and returns the new position.

Figure~\ref{fig:SequentialSimulation} gives code for a sequential simulation.
The parameter |bodies| of the class represents the astronomical bodies to
simulate.  At each step, the velocity of each body is updated based on the
position of each other body, and then the position of each body is updated.
The simulation returns an array giving the bodies' positions at each
timestep. 

%%%%%

\begin{figure}
\begin{scala}
/** A sequential simulation for bodies. */
class SequentialSimulation(bodies: Array[AstronomicalBody]){
  private val n = bodies.length

  /** Simulate for steps steps, returning an array of the bodies' positions at
    * each timestep. */
  def apply(steps: Int): Array[Array[Vector]] = {
    val result = Array.ofDim[Vector](steps,bodies.length)
    for(step <- 0 until steps){
      for(i <- 0 until n; j <- 0 until n; if i != j) 
        bodies(i).updateVel(bodies(j))
      for(i <- 0 until n) result(step)(i) = bodies(i).move()
    }
    result
  }
}
\end{scala}
\caption{A sequential simulation}
\label{fig:SequentialSimulation}
\end{figure}

%%%%%

\begin{instruction}
Study the implementation of the sequential simulation.
\end{instruction}

We now consider how to parallelise the simulation.  We will use a number of
workers, each of which is responsible for updating some of the astronomical
bodies.  However, we need to keep the workers synchronised: we will use a
barrier synchronisation at the end of each step, to ensure that all the
workers are on the same step at the same time.  

Further, we need to avoid race conditions.  The worker for a particular
body~|b| updates |b|'s position within the |move| operation.  However, a
different worker will read that position when it executes |b1.updateVel(b)| on
one of its own bodies~|b1|.  This is a race.  In particular, if our
implementation allows these operations to happen in either order, the results
will be nondeterministic: the update to |b1| might depend on either the
previous or the new position of~|b|.

To avoid such a race, we arrange for a barrier synchronisation after all the
threads have finished the calls to |updateVel|, but before any call to
|move|.  That means that each call to |updateVel| will depend on the previous
positions of the bodies, as in the sequential program. 

The resulting concurrent simulation is in
Figure~\ref{fig:ConcurrentSimulation}.  The threads perform two barrier
synchronisations on each round, splitting the round into two subrounds.  In
the first subround, threads read the positions of bodies, and update their own
bodies' velocities.  In the second subround, threads update only their own
bodies' positions.  Thus there are no races. 

%%%%%

\begin{figure}
\begin{scala}
/** A concurrent simulation for bodies. */
class ConcurrentSimulation(bodies: Array[AstronomicalBody]){
  private val n = bodies.length

  /** Simulate for steps steps, using £numWorkers£ workers, returning an array of
    * the bodies' positions at each timestep. */
  def apply(steps: Int, numWorkers: Int): Array[Array[Vector]] = {
    // Array to hold the results.
    val result = Array.ofDim[Vector](steps,bodies.length)

    // The barrier for coordinating workers.
    val barrier = new Barrier(numWorkers)

    // A single worker.
    def worker(me: Int) = thread(s"worker($me)"){
      // This worker is responsible for £bodies[start..end)£. 
      val start = me*n/numWorkers; val end = (me+1)*n/numWorkers
      for(step <- 0 until steps){
        for(i <- start until end; j <- 0 until n; if i != j)
          bodies(i).updateVel(bodies(j))
        barrier.sync(me)
        for(i <- start until end) result(step)(i) = bodies(i).move()
        barrier.sync(me)
      }
    }

    // Run the workers. 
    run(|| (for(i <- 0 until numWorkers) yield worker(i)))
    result
  }
}
\end{scala}
\caption{The concurrent simulation.}
\label{fig:ConcurrentSimulation}
\end{figure}

\begin{instruction}
Study the implementation of the concurrent simulation.
\end{instruction}

We can test the concurrent simulation using the standard technique of
comparing it to the sequential simulation.  We generate some input data at
random, run both simulations, and compare the results. 

In previous chapters, we avoided shared variables, other than read-only
variables.  We have relaxed this restriction here, but avoided races by
imposing rules on when different threads can read or write shared variables,
enforced via barrier synchronisations.  It is important to be clear about the
rules for accessing shared variables in such cases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Commented out section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% \begin{slide}
%% \heading{Particle computations}

%% We now consider the problem of simulating the evolution of a large collection
%% of $N$ particles (e.g. stars or planets) that evolve under gravity.  

%% We can do a discrete time simulation, with time quantum \SCALA{deltaT}.

%% In particular, we'll consider how to construct a concurrent program for this
%% task.

%% For each particle, we need to record its mass, position and velocity:
%% %
%% \begin{scala}
%% val mass = new Array[Double](N)
%% type Vector = (Double, Double, Double)
%% val position = new Array[Vector](N)
%% val velocity = new Array[Vector](N)
%% \end{scala}
%% \end{slide}

%% %%%%%

%% \begin{slide}
%% \heading{Some physics}

%% Particle~$i$ will exert a force on particle~$j$ of magnitude
%% $G.mass(i).mass(j)/distance^2$, where $G \approx 6.67 \times 10^{-11}$ is the
%% gravitational constant, and $distance$ is the distance between them.  This is
%% an attractive force, with direction along the vector from $position(j)$ to
%% $position(i)$.  

%% We could calculate the total force exerted on each particle by all other
%% particles, and store the results in 
%% %
%% \begin{scala}
%% val force = new Array[Vector](N)
%% \end{scala}

%% We could then update the velocity of each particle~\SCALA{i}
%% by:\footnote{assuming we have defined \SCALA{+}, \SCALA{*} and \SCALA{/} to
%%   operate over \SCALA{Vector}}
%% %
%% \begin{scala}
%% velocity(i) += deltaT*force(i)/mass(i)
%% \end{scala}

%% And we could update the position of particle~\SCALA{i} by
%% %
%% \begin{scala}
%% position(i) += deltaT*velocity(i)
%% \end{scala}
%% \end{slide}

%% %%%%%

%% \begin{slide}
%% \heading{Calculating the forces}

%% Note that the force exerted by particle $i$ on particle $j$ is the same as the
%% force exerted by particle $j$ on particle~$i$ (except in the opposite
%% direction).  For reasons of efficiency, we do not want to calculate this
%% quantity twice.  

%% What we will do is allocate each thread some set~$S$ of particles.  For each
%% particle~$i \in S$, the thread will calculate the forces between $i$ and all
%% particles~$j$ with $j>i$.  These will be added to the total forces on both $i$
%% and $j$.  Something like:
%% %
%% \begin{scala}
%% for(i <- S; j <- i+1 until N){
%%   val thisForce = ... // force exerted on i by j
%%   force(i) += thisForce
%%   force(j) -= thisForce
%% }
%% \end{scala}
%% \end{slide}

%% %%%%%

%% \begin{slide}
%% \heading{Avoiding race conditions}

%% The code on the previous slide has an obvious race condition: several
%% threads might be trying to write to the same \SCALA{force(i)}
%% simultaneously. 

%% Instead we arrange for each thread~$me$ to write to its own vector of forces.
%% %
%% \begin{scala}
%% val force1 = new Array[Array[Vector]](p,N)
%% \end{scala}
%% %
%% Something like:
%% %
%% \begin{scala}
%% for(i <- S; j <- i+1 until N){
%%   val thisForce = ... // force exerted on i by j
%%   force1(me)(i) += thisForce
%%   force1(me)(j) -= thisForce
%% }
%% \end{scala}
%% \end{slide}

%% %%%%%

%% \begin{slide}
%% \heading{Calculating the total forces}

%% Once all the \SCALA{force1} values have been calculated, the threads can
%% perform a barrier synchronisation.  

%% Then the thread with set of particles~$S$ can, for each particle~$i \in S$,
%% calculate the total force and update the velocity and position:
%% %
%% \begin{scala}
%% for(i <- S){
%%   var force: Vector = (0.0, 0.0, 0.0)
%%   for(k <- 0 until p) force += force1(k)(i)
%%   velocity(i) += deltaT*force/mass(i)
%%   position(i) += deltaT*velocity(i)
%% }
%% \end{scala}
%% %
%% We expect to have $p \ll N$, so the cost of this extra summation is
%% comparatively small. 

%% The threads can then perform another barrier synchronisation before the next
%% round. 
%% \end{slide}

%% %%%%%

%% \begin{slide}
%% \heading{The pattern of synchronisation}

%% This pattern of synchronisation is very common:
%% %
%% \begin{scala}
%% <initialisation>
%% barrier.sync
%% while(true){
%%   <read all variables>
%%   barrier.sync
%%   <write own variables>
%%   barrier.sync
%% }
%% \end{scala}
%% %
%% The final synchronisation on each iteration could be replaced by a
%% synchronisation using a combining barrier, to decide whether to continue.
%% \end{slide}

%%%%%

%% \begin{slide}
%% \heading{Load balancing}

%% We want to choose the sets $S$ allocated to different threads so as to
%% balance the total load.

%% Note that the cost of calculating all the forces for particle~$i$ is
%% $\Theta(N-i)$: not all particles are equal in this regard.

%% One way to balance the load is to split the $N$ particles into $2p$
%% segments, each of size $segSize = N/2p$.  Then we can allocate process~$me$
%% the segments~$me$ and $2p-me-1$, i.e.\ particles $[me.segSize \upto
%%   (me+1).segSize)$ and $[(2p-me-1).segSize \upto (2p-me).segSize)$.
%% \end{slide}




