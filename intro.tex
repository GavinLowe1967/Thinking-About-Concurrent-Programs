\chapter{Introduction}
\pagenumbering{arabic}

A concurrent program is any program consisting of multiple interacting tasks,
implemented as separate ``threads of control''.  We will see various patterns
for how the different tasks work together.  For example, the different tasks
might be performing different operations, as part of a pipeline; or they might
be performing the same operation, but on different parts of the data.

The tasks might all execute on the same processor, sharing processor time.  Or
they might operate on several close-coupled processors, within a single
computer.  Or they might be distributed across a network.

The key concern in each case is coordinating the different tasks, to ensure
correct sequencing of interactions or communications between them.

%%%%%%\heading{Processes and threads}

Concurrent tasks can be implemented as either \emph{processes} or
\emph{threads}.

A thread is a single sequentially executing program.  At each point, it is at
a particular place in the program, and has its own \emph{thread-local}
variables and a control stack.  However, different threads may share memory,
and so communicate with one another via shared variables.  This provides for a
low communication overhead.  Threads are normally fairly lightweight, so
context switching (where a processor switches from executing one thread to
another) can be cheap.

A process consists of one or more threads sharing an address space.  However,
different processes have separate address spaces. which protects threads from
each other.  Processes communicate with each other, either using
operating-system calls, or by communicating over a network.  This means that
they have relatively high communication overheads.

In this book we will use threads sharing a single address space.  However,
some of our programs use message passing (only), so the threads could be
replaced by processes in distinct address spaces, or even on different
computers.

The word \emph{concurrent} refers to two or more tasks that overlap in time.
However, they might not ever be running at precisely the same instance:
instead, a processor could be switching between them.  By contrast, the word
\emph{parallel} means that the tasks are running at the same time, on
different hardware.  We will not distinguish much between concurrency and
parallelism in this book.  All of our programs could run on a single
processor, time sharing.  However, we really expect them in parallel, on
different processors, so as to give faster performance. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Reasons for concurrency}

Why should we write concurrent programs?

The most obvious reason for using concurrency is that running a program on
multiple processors in parallel will often make the program faster.

\framebox{Improve}

Examples:
%
\begin{itemize}
\item
Scientific computations: e.g.~climate modelling, evolution of a galaxy,
effects of new drugs;

\item
Graphics and image processing;

\item
Combinatorial or optimisation problems: e.g.~scheduling problems, game-playing
algorithms.
\end{itemize}

Typical structures for such programs:
%
\begin{itemize}
\item
Data parallel: each thread/process works on part of the data, with all
threads/processes working in the same way;

\item
Task parallel: different threads/processes do different things.
\end{itemize}
%
% (Much more on this later.)

As we will see, using concurrency might not make a program faster.  There is a
danger that the communication and coordination costs outweigh the benefits of
using extra threads.

%%%%%

\heading{Reasons for concurrency: (2) multi-task systems}

Many systems are responsible for a collection of independent (or
semi-independent) tasks, each with its own data.  Such systems are easier to
program using one single thread or process per task, rather than having a
single monolithic thread that is responsible for all the tasks.

For example, consider a controller for a light, that turns the light on and
off periodically.  Using appropriate library code:
%
\begin{scala}
def controller(light: Light, tOn: Int, tOff: Int) = {
  private var tBase = getCurrentTime
  while(true){
    light.on; sleepUntil(tBase+tOn)
    light.off; sleepUntil(tBase+tOn+tOff); tBase = tBase + tOn + tOff
  }
}
\end{scala}

Now suppose you need to control 1000 lights.  
%
\begin{itemize}
\item
How would you do this with a sequential program?

\item
With a concurrent program it is easy: create one thread or process for each
light, and run them concurrently.
\end{itemize}


More examples:
%
\begin{itemize}
\item
Operating systems;

\item
Real-time controllers for factories, power plants, spacecraft, etc;

\item
Programs with GUIs that must respond to user actions;

\item
Simulations;

\item
Multi-character games.
\end{itemize}

In such multi-threaded multi-task systems, multiple threads or processes run
concurrently on the same processor(s), often with more threads/processes than
processors.  The threads/processes take turns to use the processor(s), under
the control of the operating system.

%%%%%

\heading{Reasons for concurrency: (3) distributed computing}

Many applications make use of services provided by computers that are
physically distributed; these are necessarily concurrent.

Examples:
%
\begin{itemize}
\item 
The Web;

\item
File servers in a network;

\item
Database systems.
\end{itemize}

These applications often use a client-server architecture.

The components might themselves be multi-threaded.

Somewhat similarly, fault-tolerant systems may use redundant components.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Concurrent architectures}

We will briefly discuss a few different concurrent architectures, to provide
background for the rest of the course.

%%%%%

\heading{Uni-processor multi-threaded systems}

Until early this century, most personal computers had a single processor.
However, they could run multiple (user and operating system) threads.

In such systems, the operating system is responsible for sharing the processor
between threads.  The operating system selects a suitable thread and
executes it until either:
\begin{itemize}
\item
the thread suspends itself, e.g.~waiting for input or output to complete; or

\item
the thread has used up its time allocation, at which point it is interrupted.
\end{itemize}
%
A new thread can then be scheduled.  See an Operating Systems textbook for
more details.

%%%%%

\heading{Thread states}

%\includegraphics[width=12cm]{Pics/threadstates.eps}

\begin{tikzpicture}
\draw(0,0) node[draw, rounded corners](runnable){Runnable};
\draw[<-] (runnable) -- node[above]{start} (-2,0);
%
\draw(6,0) node[draw, rounded corners](running){Running};
\draw[->] ([yshift = 1mm] runnable.east) -- node[above]{schedule} 
  ([yshift = 1mm] running.west);
\draw[->] ([yshift = -1mm] running.west) -- node[below]{deschedule} 
  ([yshift = -1mm] runnable.east);
%
% \draw (3,3) node[draw, rounded corners](waiting){Waiting};
% \draw[->] (running) -- node[above, sloped]{\scalashape wait} (waiting);
% \draw[->] ([xshift = -1mm] waiting.south) -- node[above, sloped]
%   {\scalashape notify} ([xshift = -1mm] runnable.north);
% \draw[->] ([xshift = 1mm] waiting.south) -- node[below, sloped]
%   {spurious wake-up} ([xshift = 1mm] runnable.north);
%
\draw (0, -3) node[draw, rounded corners](finished){Finished};
\draw[->] (runnable) -- node[below, sloped]{stop} (finished);
\draw[->] (running) -- node[above, sloped, near start]{done} (finished);
%
\draw(6, -3) node[draw, rounded corners](blocked){Blocked};
\draw[->] ([xshift = 1mm] running.south) -- 
  node[above, sloped]{block} ([xshift = 1mm] blocked.north);
\draw[->] ([xshift = -1mm] blocked.north) -- 
  node[below, sloped, near start]{unblock} 
  ([xshift = -1mm] runnable);
%
% \draw(7, 3) node[draw, rounded corners](IOblocked){IO Blocked};
% \draw[->] ([xshift = -1mm] running.north) -- 
%   node[above, sloped]{block for IO} ([xshift = -1mm] IOblocked.south);
% \draw[->] ([xshift = 1mm] IOblocked.south) -- node[below, sloped]{complete IO} 
%   ([xshift = 1mm] running.north);
\end{tikzpicture}

%%%%%

\heading{Shared-memory multiprocessors}

In shared-memory multiprocessors, several processors share one or more
memories.  They are connected by an interconnection network, e.g.\ a memory
bus.  Each processor has its own (fast) cache memory. 

\framebox{Diagram}

%% %
%% \begin{center}
%% \ \includegraphics[width=6cm]{Pics/multiprocessors.eps}\ 
%% \end{center}
% \framebox{Fig 1.2 from Andrews}
% 
The shared memory might be organised hierarchically, particularly in a system
with many processors.

%%%%%

\heading{Cache consistency}

In shared-memory multiprocessor systems, each processor has its own cache.
Values read from the main memory are copied into the cache; a subsequent read
of the same address will read that value from the cache.  If an address is
written to, that write initially happens just in the cache, and later copied
back to the main memory.

Different processors may read and write the same location.  This can be
problematic if the different caches are not consistent.  The specification of
most programming languages include a \emph{memory model} which defines the
degree of consistency that is guaranteed: see later.

%% It is therefore important to keep different caches consistent.  See the
%% Architecture course for details.

%%%%%


\heading{Multi-core processors}

Nowadays, most processors have multiple cores on the same chip.  Each core may
have its own level~1 cache, and they may share a level~2  and level-3 cache.

\framebox{Diagram}

%
%% \begin{center}
%% \includegraphics[height=32mm]{190px-Dual_Core_Generic.svg.ps} \hspace{2cm}
%% (Image from Wikipedia.)
%% \end{center}

Most current off-the-shelf computers are dual-core or quad-core; servers
typically have tens or hundreds of cores.

Multi-core processors provide little speed-up unless the software is designed
to exploit concurrency.

%%%%%

\heading{General-purpose computing on graphics processing units}

A \emph{graphics processing unit} (GPU) is a special-purpose multi-core
processor, originally designed to do graphics processing.  They were designed
to perform many operations in parallel, and so are much faster at floating
point calculations than traditional
CPUs.
%\footnote{\url{http://gpgpu-computing.blogspot.com/}} 

They originally had rather limited functionality.  However, APIs have
been developed that allow them to be used for general-purpose computing.  They
have been used to parallelise many different
applications.
%\footnote{\url{http://en.wikipedia.org/wiki/GPGPU}} 

%%%%%

\heading{Distributed-memory systems}

In distributed-memory systems, each processor has its own private memory.
Processors communicate by message passing, rather than using shared memory. 

\framebox{Diagram}
%
%% \begin{center}
%% \ \includegraphics[width=6cm]{Pics/distributed.eps}\ 
%% \end{center}
% \framebox{Fig 1.3 from Andrews}
%
They might communicate via a high-speed interconnection network, or over a
network, such as a local area Ethernet or the Internet.

%%%%%

%% \begin{slide}
%% \heading{Distributed-memory multicomputers and networks}

%% A \emph{multicomputer} is a distributed-memory system where the processors are
%% physically close, connected by a high-speed interconnection network.

%% In \emph{network systems} (sometimes known as grids), processors
%% communicate over a network, such as a local area Ethernet or the
%% Internet.

%% One can, of course, build distributed-memory systems, where each computer is
%% itself a shared-memory multiprocessor.
%% % multicomputers from shared-memory multiprocessors.
%% \end{slide}

%% %%%%%

%% \begin{slide}
%% \heading{Grid systems and cloud computing}

%% Grid systems are large network systems, often sharing resources across
%% organisations.  They come in two main flavours:
%% %
%% \begin{description}
%% \item[Computational grids,] where computers work together on some
%%   computational task.
%%   % mostly used on problems that require little communication between the
%%   % subtasks. 

%% \item[Data grids,] where there is a huge amount of data to be processed.
%%   % Example types of data include the web (e.g.~web searching), astronomical
%%   % data, data from particle accelerators, genomic sequences.  The data grid may
%%   % be combined with a computational grid that operates on the data.
%% \end{description}


%% Cloud computing can be thought of as ``grids for hire''.  Companies ---such as
%% Amazon, HP, Google, Rackspace--- make grids of computers available to be hired
%% by organisations.  Whether to own your own grid or to hire is mainly an
%% economic decision. 
%% \end{slide}

%%%%%

%% \begin{slide}
%% \heading{Example: Google}

%% Google has 15 data centres worldwide, using about 900,000 servers.
%% %  between 500,000 and several
%% % million processors in total, grouped into clusters of a few thousand
%% % processors.\footnote{{\it Data-Intensive Supercomputing: The case for DISC},
%% %   Randal E. Bryant, CMU-CS-07-128.}
%% Each data centre stores several copies of the web (about 1.2 million terabytes)
%% % (about 200 terabytes),
%% together with indexes to allow efficient searching.  This is continually
%% updated by processes that crawl the web.

%% They use their own custom file system\footnote{{\it The Google File System}, 
%% Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung,
%% \url{http://labs.google.com/papers/gfs.html}.}, to distribute the data across
%% servers, providing high performance and fault tolerance. 

%% Each web search uses about 1000 computers, in order to complete the search in
%% about 0.2 seconds.  

%% % Each web search requires about 10 seconds of computer time, but is completed
%% % in about 0.1 seconds by using multiple processors.

%% Their design uses hardware that is low-cost and low-power, rather than fast or
%% reliable. 

%% \vfill
%% \end{slide}

We will mainly consider programs for shared-memory multiprocessors in this
book.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Disciplined interaction}

The main challenge of concurrent programming is ensuring disciplined
interaction between threads.  Without this, even very simple concurrent
programs can act in unexpected ways.  In particular, it is necessary to ensure
disciplined access to shared variables.

Figure~\ref{fig:race} gives an example that shows what can happen if you allow
undisciplined access to variables.  Such programs are very hard to reason
about.
Appendix~\ref{app:scala} contains a brief introduction to the Scala
programming language.  We will concentrate below on the concurrency aspects of
the program. 

%%%%%

\begin{figure}
\begin{scala}[numbers = left]
import ox.scl._  £\label{line:input}£

/** Program to show the dangers of undisciplined shared variables. */
object Race{
  var x = 0

  /** Thread to increment £x£ 1000 times. */
  def t1 = thread{ for(i <- 0 until 1000) x = x+1 }£\label{line:p}£

  /** Thread to decrement £x£ 1000 times. */
  def t2 = thread{ for(i <- 0 until 1000) x = x-1 }£\label{line:q}£

  /** Parallel composition. */
  def system = t1 || t2£\label{line:system}£

  def main(args: Array[String]) = { run(system); println(x) }£\label{line:main}£
}
\end{scala}% File Race/Race.scala
\caption{A simple program exhibiting a memory race.}
\label{fig:race}
\end{figure}

%%%%%


We use the Scala Concurrency Library (SCL) in this book.  It provides a number
of convenient concurrency primitives.  Line~\ref{line:input} of
Figure~\ref{fig:race} imports this library, so the main primitives can be used
directly; for example, ``|thread|'' is shorthand for ``|ox.scl.thread|''.  (In
later programs, we will tend to omit this |import| statement, for brevity.)

A definition of the form \SCALA{thread\{<code>\}} defines a thread that when
executed will execute \SCALA{<code>}.  For example, line~\ref{line:p} defines
|t1| to be a thread that when executed will increment the shared variable~|x|
1000 times, and line~\ref{line:q} defines |t2| to be a thread that when
executed will decrement |x| 1000 times.  Each thread is of type
|ThreadGroup|\footnote{This type is not the same as the type {\scalashape
    java.lang.ThreadGroup}.}.

If |p| and |q| are |ThreadGroup|s, then \SCALA{p || q} represents their
parallel composition, also of type |ThreadGroup|.  For example,
line~\ref{line:system} defines |system| to be the parallel composition of~|t1|
and~|t2|.  A parallel composition terminates when both components have
terminated.

If |p| is a |ThreadGroup|, then |run(p)| or |p.run| runs~|p|.  Thus the |main|
function at line~\ref{line:main} runs |system|, i.e.~runs |t1| and~|t2| in
parallel, and prints the final value of~|x|.

You might expect that the increments and decrements cancel out, so that the
program always prints~|0|.  In fact, it can give any result between $-1000$
and $+1000$.
%
To understand why, we need to understand how a concurrent program is
executed. 
 
Certain actions can be considered \emph{atomic}; for example a single machine
instruction, or, in the case of a Scala program, a single instruction of the
Java Virtual Machine.  Typical actions include loading a variable into a
register, incrementing or decrementing it, or storing a register into a
variable. 

A sequential program, or a single thread, executes a sequence of atomic
actions.  
%
A concurrent program runs two or more sequential programs: the atomic actions
of the sequential programs are interleaved.

Let's consider a slightly simpler program, where |t1| increments~|x| just
once, and |t2| decrements~|x| just once.  Let's suppose each does this via three
instructions, loading~|x| into a register, incrementing or decrementing it,
and storing the result back in~|x|.  We could write this as
\begin{quote}
\SCALA{LD x; INC; ST x} \qquad and \qquad \SCALA{LD x; DEC; ST x}
\end{quote}
respectively (in fact, the JVM uses slightly longer sequences).

These six instructions can be interleaved in multiple ways.  The table below
considers three interleavings, starting from $\sm x = 0$.
%
\begin{center}
\begin{tabular}{lccccccl}
|x = x+1| & \SCALA{LD x} & \SCALA{INC} & \SCALA{ST x} & \\
|x = x-1| &      &      &     & \SCALA{LD x} & \SCALA{DEC} & \SCALA{ST x}
& ($\sm x = 0$) 
\\
\hline
\SCALA{x = x+1} & \SCALA{LD x} &      & \SCALA{INC} & \SCALA{ST x} & \\
\SCALA{x = x-1} &      & \SCALA{LD x} &     &      & \SCALA{DEC} & \SCALA{ST x}
& (\SCALA{x = -1}) 
\\
\hline
\SCALA{x = x+1} & \SCALA{LD x} &      &     &      & \SCALA{INC} & \SCALA{ST x} & \\
\SCALA{x = x-1} &      & \SCALA{LD x} & \SCALA{DEC} & \SCALA{ST x} &     &
& (\SCALA{x = 1})
\end{tabular}
\end{center}
%
In the first interleaving, all the atomic instructions of the increment are
executed before all the atomic instructions of the decrement, and so we end up
with $\sm x = 0$.  In the second interleaving, both threads read the initial
value of~|x|, then the first thread writes the result of the increment ($1$),
and then the second thread writes the result of the decrement ($-1$),
overwriting the earlier write; hence we end up with $\sm x = -1$.  In the
third interleaving, the writes happen in the opposite order, and so we end up
with $\sm x = 1$.  All other interleavings give one of these three values.  

Returning to the program in Figure~\ref{fig:race}, it should be clear how
different interleavings can produce any result between $-1000$ and~$1000$.

More generally, two actions are \emph{independent} if their order may be
reversed without changing the overall effect.  For example, two actions
concerning distinct variables are independent, as are two reads of the same
variable.  Likewise, the |INC| and |DEC| actions by the above threads affect
only thread-local registers, so are independent of any other actions.
Interleaving of independent atomic actions is not problematic.

However, some actions are non-independent; for example two writes of the same
variable, or a read and a write of the same variable.  Interleaving of
non-independent atomic actions can be problematic.

We say that a program contains a \emph{race} if two non-independent actions
can be interleaved, and one of the interleavings leads to an incorrect
result.  In particular, where those actions are reads or writes of shared
variables, we say that it is a \emph{memory race}.  The program in
Figure~\ref{fig:race} contains memory races, as exhibited by the second and
third interleavings above. 

Different threads can run at different rates: threads can be descheduled and
only later rescheduled; threads may be delayed in memory actions because of
congestion on the memory bus.  It is not feasible to predict which
interleaving will occur.


%%%%% \heading{Caching}

However, there are other reasons why programs with memory races can be
unpredictable.  
%
As described earlier, multiprocessor machines may cache variables.  Caching
can make a program much faster; but it can create problems with concurrent
programs.

When a thread~$t$ first needs the value of a variable~|x|, it will read it
from shared memory, and store a copy in its cache.  If it later needs the
value of~|x|, it may read it from its cache.  However, if some other thread
has updated~|x| in the mean time, thread~$t$ will not see the result of this
update!

Similarly, when a thread~$t$ updates a variable, that update is initially made
only in its cache.  If another thread reads the variable, it will not see the
result of the update.

In the program of Figure~\ref{fig:race}, each thread might read the initial
value of~|x|, perform updates only within its cache, and write the final
value, $1000$ or $-1000$, only at the end.  (However, they are guaranteed to
write their final value when they terminate.)  Thus the final value will be
$1000$ or $-1000$, depending upon which thread writes its value last --- in
fact, on some architectures this seems to happen on most runs of the program.

%%%%% \heading{Compiler optimisations}

Another reason why programs with memory optimisations can be unpredictable
concerns memory optimisations.  The compiler may optimise code, according to
certain rules, to something that is equivalent when run sequentially.
Compiler optimisations are certainly useful, and often produce faster code.
However, the resulting code might not be equivalent when run as part of a
concurrent program.

Consider the code in Figure~\ref{fig:busyWait}.  This uses a variant of the
|thread| function: the construct |thread(name){<code>}| acts like
|thread{<code>}|  but gives the name |name| to the thread; we will see how
this is useful shortly.

In Figure~\ref{fig:busyWait}, it appears that |t1| sets |answer| to |42|, and
then sets |done| to |true|, while |t2| waits for |done| to become true before
reading |answer|: it appears that |t1| is using the variable |done| to signal
to |t2| that it has written to |answer|.

\begin{figure}
\begin{scala}
object BusyWait{
  var answer = 0

  var done = false

  /** Thread to set £answer£ to 42, and then signal to £t2£. */
  def t1 = thread("t1"){ answer = 42; done = true }

  /** Thread to wait for a signal, and read £answer£. */
  def t2 = thread("t2"){
    while(!done){ } // Busy wait.  Don't do this!!!
    val myAnswer = answer; assert(answer == 42)
  }

  /** Parallel composition. */
  def system = t1 || t2

  def main(args: Array[String]) = { 
    for(i <- 0 until 10000){ answer = 0; done = false; run(system); print(".") }
  }
}
\end{scala}
\caption{A program that shows an incorrect signalling technique.}
\label{fig:busyWait}
\end{figure}

However, the code might not act as expected because of compiler
optimisations.  For example, it would be perfectly valid for the compiler to
rewrite the body of~|t1| to
\begin{scala}
  done = true; answer = 42
\end{scala}
%
This is equivalent to the original code as a sequential program, and it a
valid rewrite.  However, the logic around the signal no longer holds.
Likewise, it would be valid to rewrite the body of~|t2| to
\begin{scala}
  val myAnswer = answer; assert(answer == 42)
  while(!done){ } 
\end{scala}
and again the logic around the signal doesn't hold.  Alternatively, it would
be valid for the compiler to rewrite the body of~|t2| to
% 
\begin{scala}
  if(!done){ while(true){ } }
  val myAnswer = answer; assert(answer == 42)
\end{scala}
This is a fairly standard form of rewrite: it avoids rereading the variable
|done|.  Now, this thread can read |done = false| and loop forever, so the
program gets stuck.  In fact, on my machine, the program runs for a few
hundred iterations and then gets stuck: I think the just-in-time compiler
(which optimises Java bytecode to native machine code, while the program is
running) performs an optimisation equivalent to the one above.

When a program does get stuck in this way, typing \texttt{Ctrl}+$\backslash$
(i.e.~holding down the \texttt{Ctrl} key, and pressing the backslash key) in
the terminal produces a thread dump, listing information about all the running
threads, such as giving their current line number in the code.  (This normally
includes several threads that are part of the runtime, rather than the program
in question; these can normally be ignored.)  In particular, this thread dump
uses names given to threads by the |thread(name){...}| construct.  In the
example of the previous paragraph, |t1| is not listed, because it has
terminated; however, |t2| is still at the line corresponding to the |while|
loop.

The style of the loop in~|t2|, where the thread spins, waiting for a condition
to become true, is known as \emph{busy waiting}.  This is normally considered
bad style.  As the above example shows, it often doesn't work!  And even if it
did work, the spinning thread would be consuming computational resources that
might be better used elsewhere.  It would be better for the thread to suspend
until the condition becomes true: we will see in later chapters how to do
this.  

We have seen that programs that contain race conditions are likely to be very
hard to reason about, unpredictable --- and wrong!  However, we want the
behaviour of our programs to be predictable.  We therefore need to design our
programs to avoid race conditions.  In particular, in order to avoid memory
races, we will design our programs so that two threads may not perform
interleaved actions on a shared variable, except if both threads are
performing reads.  We will see that this has the additional benefit of
removing the types of errors introduced by caching or compiler optimisations

%% NSA have issued a report saying that race conditions are the eighth most
%% dangerous programming error.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\heading{Correctness properties} 

\begin{description}
\item[Safety/correctness:] The results produced by the program are correct.
  Typically this requires that the system state will satisfy an (intended)
  invariant property after every ``action''.

\item[Liveness/progress:] The system as a whole does something useful.
\end{description}

\bigskip

\heading{Performance properties}

\begin{description}
\item[Latency:]
Requests get serviced reasonably quickly.

\item[Throughput:]
The system deals with a high number of requests.
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Summary}

\begin{itemize}
\item
Processes and threads;

\item
Reasons for concurrency;

\item
Concurrent architectures;

\item
Shared variable programs; 

\item
Basics of SCL;

\item
Independent and non-independent actions, race conditions, disciplined
interaction; 

\item
Desirable properties.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exercises
%% \section*{Exercises}
%% \markright{Exercises}

\input{Exercises/web-browser} % Web browser question. 

\input{Exercises/interleavings} % Count number of interleavings.

\input{Exercises/account} % Account question.

\input{Exercises/stackRace} % Stack using List

\input{Exercises/queueRace} % Queue as linked list



